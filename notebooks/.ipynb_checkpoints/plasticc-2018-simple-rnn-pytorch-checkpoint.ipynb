{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "bb773a25117ef0c50c245fe09fa32fe760ad8f6c"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1421705, 7848]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-230a39750e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'plasticc_train_metadata.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[1;32m    298\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1421705, 7848]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_csv(DATA_DIR+'plasticc_train_lightcurves.csv')\n",
    "train_meta = pd.read_csv(DATA_DIR+'plasticc_train_metadata.csv')\n",
    "\n",
    "train, test = train_test_split(train, train_meta, test_size=0.2)\n",
    "\n",
    "train_meta.shape, test_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ee3482779ac68635e537827076465238377057d"
   },
   "outputs": [],
   "source": [
    "target = train_meta['target'].values.copy()\n",
    "labels2weight = {x:1 for x in np.unique(target)}\n",
    "train_mask = train_meta['distmod'].isnull().values #galactic\n",
    "test_mask  = test_meta['distmod'].isnull().values\n",
    "\n",
    "labels2weight[15] = 2\n",
    "labels2weight[64] = 2\n",
    "labels2weight[99] = 2\n",
    "\n",
    "import collections\n",
    "target2y = dict(map(reversed, enumerate(np.unique(target))))\n",
    "y2target = dict(enumerate(np.unique(target)))\n",
    "y = np.array(list(map(target2y.get, target)))\n",
    "class_weight = np.array(list(map(lambda x: labels2weight[y2target[x]], sorted(np.unique(y)))))\n",
    "y_cntr = collections.Counter(y)\n",
    "wtable = np.array([y_cntr[i] for i in sorted(np.unique(y))]) / len(y)\n",
    "\n",
    "print(sorted(np.unique(y)))\n",
    "print(wtable)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8d4594a319e4036d16b6f5f92472763356ec20c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "nfolds = 5\n",
    "kf = KFold(n_splits=nfolds, shuffle=True, random_state=42)\n",
    "cv_folds = np.arange(len(target))\n",
    "for i,_ in enumerate(kf.split(train_meta, target)):\n",
    "    cv_folds[_[1]] = i\n",
    "evals = pd.DataFrame()\n",
    "evals['object_id'] = train_meta['object_id']\n",
    "evals['target'] = target\n",
    "evals['cv_folds'] = cv_folds\n",
    "evals['is_gal'] = train_mask.astype('int')\n",
    "evals['is_ddf'] = train_meta['ddf'].values\n",
    "evals.to_csv('evals.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "05373387db4736c58cfe6963c88fc72106de599a"
   },
   "outputs": [],
   "source": [
    "remove_cols = ['hostgal_specz', 'target']\n",
    "for c in remove_cols:\n",
    "    if c in train_meta.columns:\n",
    "        del train_meta[c]\n",
    "    if c in test_meta.columns:\n",
    "        del test_meta[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bdf61fa0200fbd457545daf61ecb338d8189af8b"
   },
   "outputs": [],
   "source": [
    "train_meta['distmod'].fillna(0, inplace=True)\n",
    "test_meta['distmod'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9f0c48da0add450503dbb4cbcc11ad09eb6fd60"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 4], dpi=90)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['flux'].sample(frac=0.1))\n",
    "plt.title('flux raw distribution')\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(train['flux_err'].sample(frac=0.1))\n",
    "plt.title('flux_err raw distribution')\n",
    "plt.grid();\n",
    "\n",
    "plt.figure(figsize=[12, 4], dpi=90)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.distplot(train['flux'].sample(frac=0.1).apply(lambda x: np.sign(x) * np.log(np.abs(x))))\n",
    "plt.title('flux symlog distribution')\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.distplot(train['flux_err'].sample(frac=0.1).apply(lambda x: np.sign(x) * np.log(np.abs(x))))\n",
    "plt.title('flux_err symlog distribution')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1c36d0f90e0d332b07ac2344581c08737718f19"
   },
   "outputs": [],
   "source": [
    "for c in ['flux', 'flux_err']: \n",
    "    train[c] = train[c].apply(lambda x: np.sign(x) * np.log(np.abs(x)))\n",
    "c = 'mjd'\n",
    "train[c] = (train[c] - train[c].mean()) / train[c].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e119474382fc6b05d1c113a9b3167993ed32d94"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train = train.groupby(['object_id', 'passband']).apply(\n",
    "    lambda x: x.set_index(['object_id', 'passband']).to_dict(orient='list')\n",
    ")\n",
    "train.to_pickle('train_ts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2cc553e768e86801971c3e1965fd3856e69ec67"
   },
   "outputs": [],
   "source": [
    "print(train.loc[615, 0].keys())\n",
    "train.to_frame().head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "503b8de48dd277a2075a2eea90b22f7a74418c88"
   },
   "outputs": [],
   "source": [
    "train_ids = train_meta['object_id'].values\n",
    "train_meta = train_meta.set_index('object_id')\n",
    "train_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b18d37fa261fe6eb0ccfff23b49ddb47ed6e7fb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8ed39220d133e7a121c72f4a986aef293a37bb8"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a2552de353d5ffc1826156044890670b8388003"
   },
   "outputs": [],
   "source": [
    "meta_cols = []\n",
    "meta_cols+= ['ra', 'decl', 'gal_l', 'gal_b']\n",
    "meta_cols+= ['ddf', 'hostgal_photoz', 'hostgal_photoz_err', 'distmod', 'mwebv']\n",
    "\n",
    "def get_xs_by_idx(idx, data):\n",
    "    xs = [pd.DataFrame(data[idx, pb]).values for pb in range(6)]\n",
    "    return xs\n",
    "\n",
    "def get_meta_by_idx(idx, metadata):\n",
    "    return train_meta.loc[idx, meta_cols].values\n",
    "\n",
    "def get_ts_mt_by_ids(ids, tsdata, metadata):\n",
    "    ts = [[] for pb in range(6)]\n",
    "    mt = []\n",
    "    for _id in ids:\n",
    "        xs = get_xs_by_idx(_id, tsdata)\n",
    "        for i,x in enumerate(xs):\n",
    "            ts[i].append(x)\n",
    "        mt.append(get_meta_by_idx(_id, metadata))\n",
    "    return ts, mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "006059ecaf44d3a70edd4459f2409674b0107c27"
   },
   "outputs": [],
   "source": [
    "valid_fold = 0\n",
    "\n",
    "num_class = int(y.max()+1)\n",
    "num_rnn_unit = 32\n",
    "num_rnn_layer = 2\n",
    "dropout_rnn = 0.25\n",
    "num_linear = 64\n",
    "\n",
    "lr = 0.0009\n",
    "weight_decay = 0\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffa9e85fd9353b5a6bbc08e80a12b635e1f0bba1"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, 1).reshape(-1, 1))\n",
    "    return e_x / e_x.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "def loss_fn(preds, target, num_class=num_class, class_weight=class_weight, wtable=wtable):\n",
    "    class_weight = torch.from_numpy(class_weight).type(preds.type())\n",
    "    wtable = torch.from_numpy(wtable).type(preds.type())\n",
    "    y_ohe = torch.zeros(\n",
    "        target.size(0), num_class, requires_grad=False\n",
    "    ).type(preds.type()).scatter(1, target.reshape(-1, 1), 1)\n",
    "    preds = F.softmax(preds, dim=1)\n",
    "    preds = torch.clamp(preds, 1e-15, 1-1e-15)\n",
    "    prod = torch.sum(torch.log(preds) * y_ohe, dim=0)\n",
    "    prod = prod * class_weight / wtable / target.size(0)\n",
    "    loss = -torch.sum(prod) / torch.sum(class_weight)\n",
    "    return loss\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, RNN=nn.GRU, use_cuda=torch.cuda.is_available()):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.rnn = RNN(\n",
    "            4, num_rnn_unit, num_rnn_layer, \n",
    "            batch_first=True, bidirectional=True, dropout=dropout_rnn\n",
    "        )\n",
    "        \n",
    "    def forward(self, li):\n",
    "        lens = [_.shape[0] for _ in li]\n",
    "        indices = np.argsort(lens)[::-1].tolist()\n",
    "        rev_ind = [indices.index(i) for i in range(len(indices))]\n",
    "        x = [torch.from_numpy(li[i]).float() for i in indices]\n",
    "        x = pad_sequence(x, batch_first=True)\n",
    "        x = Variable(x)\n",
    "        if self.use_cuda:\n",
    "            x = x.to(device)\n",
    "        input_lengths = [lens[i] for i in indices]\n",
    "        packed = pack_padded_sequence(x, input_lengths, batch_first=True)\n",
    "        ro,_ = self.rnn(packed)\n",
    "        ro,_ = pad_packed_sequence(ro, batch_first=True)\n",
    "        ro = torch.transpose(ro, 1, 2)\n",
    "        res = F.max_pool1d(ro, ro.size(2)).squeeze()\n",
    "        return res[rev_ind, :].contiguous()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 use_cuda=torch.cuda.is_available(), \n",
    "                 num_class=num_class):\n",
    "        super(Net, self).__init__()\n",
    "        self.use_cuda = use_cuda\n",
    "        for i in range(6):\n",
    "            self.add_module(f't{i}', EncoderRNN(nn.GRU))\n",
    "        self.clf_in = num_rnn_unit * 2 * 6 + len(meta_cols)\n",
    "        self.clf_ts = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.clf_in),\n",
    "            nn.Linear(self.clf_in, num_linear),\n",
    "            nn.BatchNorm1d(num_linear),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(num_linear, num_linear),\n",
    "            nn.BatchNorm1d(num_linear),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(num_linear, num_class)\n",
    "        )\n",
    "                \n",
    "    def forward(self, ts, m):\n",
    "        m = torch.from_numpy(np.array(m)).float()\n",
    "        m = Variable(m)\n",
    "        if self.use_cuda:\n",
    "            m = m.to(device)\n",
    "        x = torch.cat([getattr(self, f't{i}')(ts[i]) for i in range(len(ts))] + [m], 1)\n",
    "        logit = self.clf_ts(x)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9155867322678eea0bcbfbd3e4fc8bd57d6bffe9"
   },
   "outputs": [],
   "source": [
    "print('Checking...')\n",
    "indices = train_ids[:batch_size]\n",
    "bx, bm = get_ts_mt_by_ids(indices, train, train_meta)\n",
    "by = [y[idx] for idx in range(batch_size)]\n",
    "\n",
    "by = torch.LongTensor([y[idx] for idx in range(batch_size)])\n",
    "by = Variable(by).to(device)\n",
    "print('by.type', by.type(), 'by.size', by.size(), 'bx length:', len(bx))\n",
    "\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "pred = model(bx, bm)\n",
    "print('pred.size', pred.size())\n",
    "\n",
    "loss = loss_fn(pred, by)\n",
    "loss.backward()\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec752e96319d505962d7a32ab2cc5db84aa6d759"
   },
   "outputs": [],
   "source": [
    "trn_ids = train_ids[cv_folds!=valid_fold]\n",
    "trn_lbl = y[cv_folds!=valid_fold]\n",
    "val_ids = train_ids[cv_folds==valid_fold]\n",
    "val_lbl = y[cv_folds==valid_fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1eb9281e6adfc55e3fc96adc5a177dee97e26350"
   },
   "outputs": [],
   "source": [
    "class Dset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_ids, labels):\n",
    "        super(Dset, self).__init__()\n",
    "        self.data_ids = data_ids\n",
    "        self.labels = labels\n",
    "        self._len = len(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx = self.data_ids[index]\n",
    "        y_i = self.labels[index]\n",
    "        return idx, y_i\n",
    "    \n",
    "def collate_fn(batch, tsdata=train, metadata=train_meta):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    for _ in batch:\n",
    "        indices.append(_[0])\n",
    "        labels.append(_[1])\n",
    "    bx, bm = get_ts_mt_by_ids(indices, tsdata, metadata)\n",
    "    by = torch.from_numpy(np.array(labels)).long()\n",
    "    return bx, bm, by\n",
    "\n",
    "train_steps = int(np.ceil(len(trn_ids) / batch_size))\n",
    "valid_steps = int(np.ceil(len(val_ids) / batch_size))\n",
    "\n",
    "train_set = Dset(trn_ids, trn_lbl)\n",
    "valid_set = Dset(val_ids, val_lbl)\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_set, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print('batch_size', batch_size, 'epochs', epochs)\n",
    "print('train_steps', train_steps, 'valid_steps', valid_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "15332d8189e965c400c98d5173b6b7de29f86c00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_li = []\n",
    "val_loss_li = []\n",
    "\n",
    "val_loss = None\n",
    "pred_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d63c72970fae68add9278ec673162b9163e775fc"
   },
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e9ffff204e5054e4e2f7a06353731ab197ed90c6"
   },
   "outputs": [],
   "source": [
    "for epoch_i in range(epochs):\n",
    "    #print(f'Training epoch {epoch_i+1}')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    gen = train_loader if not verbose else tqdm.tqdm_notebook(train_loader, total=train_steps)\n",
    "    losses = 0\n",
    "    \n",
    "    for bx,bm,by in gen:\n",
    "        model.train()\n",
    "        by = Variable(by).to(device)\n",
    "        pred = model(bx, bm)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(pred, by)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += float(loss) * int(by.size(0))\n",
    "    losses = losses / len(train_loader.dataset.labels)\n",
    "    loss_li.append(losses)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    losses = 0\n",
    "    for bx,bm,by in valid_loader:\n",
    "        model.eval()\n",
    "        y_true.extend(by.numpy())\n",
    "        by = Variable(by).to(device)\n",
    "        pred = model(bx, bm)\n",
    "        loss = loss_fn(pred, by)\n",
    "        y_pred.extend(pred.cpu().data.numpy())\n",
    "        losses += float(loss) * int(by.size(0))\n",
    "    \n",
    "    losses = losses / len(valid_loader.dataset.labels)\n",
    "    y_true = np.stack(y_true)\n",
    "    y_pred = np.stack(y_pred)\n",
    "    \n",
    "    star = ' '\n",
    "    if val_loss is None:\n",
    "        pred_val = y_pred.copy()\n",
    "        val_loss = losses\n",
    "        star = '*'\n",
    "    elif losses < val_loss:\n",
    "        pred_val = y_pred.copy()\n",
    "        val_loss = losses\n",
    "        star = '*'\n",
    "    else:\n",
    "        pass\n",
    "    val_loss_li.append(losses)\n",
    "    toc = time.time() - t0\n",
    "    print(f'Epoch {epoch_i+1:>2} | valid loss {val_loss_li[-1]:.4f}{star} in {toc:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64d84053da8a5047829a884f6134fd0dc78d79a0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[6, 4], dpi=90)\n",
    "plt.plot(loss_li)\n",
    "plt.plot(val_loss_li)\n",
    "plt.grid()\n",
    "plt.legend(['train', 'valid']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b38f7a507352587bba5493cf7f3be0992651ffe9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "'''\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    #print(cm)\n",
    "    plt.figure(figsize=[10, 8], dpi=90)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "pred_val_lbl = np.argmax(softmax(pred_val), axis=1)\n",
    "classes = list(target2y.keys())\n",
    "cm = confusion_matrix(y_true, pred_val_lbl)\n",
    "plot_confusion_matrix(cm, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3f64298fff671d216f18b55220817045d5a72b3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
